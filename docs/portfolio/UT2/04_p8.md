---
title: "PrÃ¡ctica 8"
date: 2025-01-01
---

# **PrÃ¡ctica 8: **

- [Consigna](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/)
- [Google Colab](https://colab.research.google.com/drive/1uWveacy4vzu3EJz2kf9Ad6bTHAgb7Frt?usp=sharing)
- [Google Colab complementario](https://colab.research.google.com/drive/1-n2TNPnsnNhaigl03stzfFrtO0qc9miF?usp=sharing)

## **ğŸ§® Actividad: Explorar experimentacion**

## AnÃ¡lisis de Resultados â€” Entrenamiento con Backpropagation y Optimizador Adam

En este experimento se implementÃ³ una **red neuronal multicapa (MLP)** entrenada sobre el conjunto de datos **CIFAR-10**, con el objetivo de comprender el funcionamiento del algoritmo de **Backpropagation** y evaluar el impacto de los hiperparÃ¡metros del optimizador **Adam** en el proceso de aprendizaje.

### ConfiguraciÃ³n experimental

La red se construyÃ³ bajo un modelo **Sequential** compuesto por **tres capas densas de 32 neuronas cada una**, con funciÃ³n de activaciÃ³n **ReLU** y una capa de salida **softmax** de 10 neuronas, correspondiente a las clases del dataset. El optimizador empleado fue **Adam**, configurado con *learning rate* de **0.001**, parÃ¡metros por defecto *(Î²â‚=0.9, Î²â‚‚=0.999)*, y una funciÃ³n de pÃ©rdida de tipo **sparse_categorical_crossentropy**.

El entrenamiento se realizÃ³ durante **5 Ã©pocas**, con un **batch size de 128**, y utilizando el conjunto de prueba como validaciÃ³n temporal. En total, el modelo contÃ³ con **100.778 parÃ¡metros entrenables**.

### Resultados obtenidos

Tras el entrenamiento, los resultados fueron los siguientes:

- **Accuracy en entrenamiento:** 51.1%
- **Accuracy en test:** 46.6%
- **Gap de generalizaciÃ³n:** aproximadamente **4.5 puntos porcentuales**

El rendimiento alcanzado se encuentra dentro de los rangos esperados para una red MLP aplicada a CIFAR-10, un dataset visual complejo que suele requerir **arquitecturas convolucionales (CNNs)** para capturar correctamente las caracterÃ­sticas espaciales. La diferencia moderada entre el rendimiento de entrenamiento y prueba sugiere una leve presencia de **overfitting**, aunque el nivel general de accuracy tambiÃ©n indica **cierto underfitting**, producto de la **baja capacidad del modelo y el nÃºmero reducido de Ã©pocas**.

### InterpretaciÃ³n y anÃ¡lisis

El comportamiento observado refleja un **aprendizaje efectivo pero incompleto**: el modelo logra captar ciertos patrones bÃ¡sicos, pero no alcanza a representar la complejidad del espacio de imÃ¡genes tridimensionales aplanadas (32Ã—32Ã—3 â†’ 3072 features).

Esto puede deberse a varios factores:

- **Capacidad limitada** del modelo (pocas neuronas por capa).
- **Cantidad insuficiente de Ã©pocas**, que restringe la convergencia de los gradientes.
- Falta de **mecanismos de regularizaciÃ³n** (Dropout, L2 o BatchNormalization) que estabilicen el entrenamiento y mejoren la generalizaciÃ³n.
- **Uso del conjunto de test como validaciÃ³n**, lo que impide una evaluaciÃ³n mÃ¡s precisa del desempeÃ±o real fuera de muestra.

### Propuestas de mejora

En futuras iteraciones, se plantea:

1. Implementar un **conjunto de validaciÃ³n independiente** y callbacks como *EarlyStopping* y *ReduceLROnPlateau* para optimizar el proceso de entrenamiento.
2. Introducir **BatchNormalization** y **Dropout (0.2â€“0.3)** entre capas densas para mejorar la estabilidad y prevenir el sobreajuste.
3. Aplicar **regularizaciÃ³n L2 (1e-4)** sobre los pesos y aumentar el nÃºmero de Ã©pocas (15â€“30) para permitir una convergencia mÃ¡s completa.
4. Experimentar con **tamaÃ±os de batch menores (64)**, que suelen favorecer la generalizaciÃ³n, y probar **otros optimizadores** (SGD con momentum o AdamW) para contrastar comportamiento y velocidad de convergencia.
5. Incorporar mÃ©tricas adicionales como **matriz de confusiÃ³n y F1-score** para identificar las clases mÃ¡s confundidas (por ejemplo, *cat* vs *dog* o *automobile* vs *truck*), aportando una visiÃ³n mÃ¡s profunda del rendimiento por categorÃ­a.

### ConclusiÃ³n

En sÃ­ntesis, el modelo logrÃ³ un rendimiento inicial satisfactorio considerando su simplicidad, demostrando el funcionamiento correcto del algoritmo de **Backpropagation** y la eficacia del optimizador **Adam** en la actualizaciÃ³n de pesos.

No obstante, el resultado evidencia la necesidad de **incrementar la capacidad del modelo y aplicar tÃ©cnicas de regularizaciÃ³n y ajuste de hiperparÃ¡metros** para mejorar la generalizaciÃ³n. Este ejercicio permite comprender de forma prÃ¡ctica cÃ³mo la arquitectura, el tamaÃ±o del batch, la tasa de aprendizaje y los callbacks influyen en la dinÃ¡mica del aprendizaje profundo, constituyendo una base sÃ³lida para futuras exploraciones con redes convolucionales o arquitecturas mÃ¡s avanzadas.

## AnÃ¡lisis del efecto de la arquitectura en el desempeÃ±o del modelo

![.](../../assets/image_UT2_p8.png)

El grÃ¡fico representa la relaciÃ³n entre la **complejidad del modelo** (medida por la cantidad total de parÃ¡metros entrenables) y la **precisiÃ³n alcanzada (accuracy)** tanto en el conjunto de **validaciÃ³n** como en el de **test**, dentro del marco de redes multicapa (MLP) aplicadas al dataset **CIFAR-10**.

### Observaciones principales

1. **Tendencia general positiva hasta cierto punto**
    
    Se observa que, a medida que aumenta la cantidad de parÃ¡metros (lo que implica arquitecturas con mayor nÃºmero de neuronas o capas), la accuracy mejora de manera sostenida hasta alcanzar un valor mÃ¡ximo cercano al **0.50 (50%)**. Esto indica que una **mayor capacidad del modelo** permite capturar patrones mÃ¡s complejos de los datos, reduciendo el error de entrenamiento y mejorando la generalizaciÃ³n inicial.
    
2. **SaturaciÃ³n y leve decrecimiento final**
    
    A partir de un determinado tamaÃ±o de modelo (la parte derecha del grÃ¡fico), el incremento en parÃ¡metros **no se traduce en mejoras adicionales**, e incluso se percibe una **ligera caÃ­da en la accuracy de test**. Esto sugiere que el modelo puede haber comenzado a **sobreajustarse (overfitting)** a los datos de entrenamiento, perdiendo capacidad de generalizaciÃ³n.
    
    Este fenÃ³meno es tÃ­pico en MLPs con muchas neuronas o capas cuando el dataset es complejo y de alta dimensionalidad como CIFAR-10, especialmente sin tÃ©cnicas de regularizaciÃ³n (Dropout, L2, BatchNorm).
    
3. **Comportamiento similar entre validaciÃ³n y test**
    
    Las curvas de validaciÃ³n y test se mantienen **muy prÃ³ximas**, lo que indica que las diferencias observadas no son producto del azar ni de un sesgo de particiÃ³n de datos.
    
    En tÃ©rminos prÃ¡cticos, esto valida que los resultados son **consistentes** y reflejan un patrÃ³n real del desempeÃ±o del modelo frente a la complejidad de la arquitectura.
    
4. **Variabilidad intermedia (inestabilidad)**
    
    En las arquitecturas mÃ¡s pequeÃ±as, la accuracy presenta cierta oscilaciÃ³n: algunos modelos con menos parÃ¡metros rinden igual o mejor que otros mÃ¡s grandes. Esto puede deberse a **efectos de inicializaciÃ³n** o **interacciones entre activaciones y tasa de aprendizaje**, que afectan la convergencia del entrenamiento cuando el modelo es poco profundo.
    

---

### InterpretaciÃ³n general

El grÃ¡fico confirma la **existencia de un punto de equilibrio** entre la capacidad del modelo y su habilidad para generalizar.

- Los modelos **demasiado simples** (pocos parÃ¡metros) no logran capturar patrones relevantes â†’ **underfitting**.
- Los modelos **excesivamente complejos** (demasiados parÃ¡metros) tienden a memorizar los datos â†’ **overfitting**.
- El mejor rendimiento se alcanza en una regiÃ³n intermedia, donde el modelo dispone de la capacidad suficiente sin perder generalizaciÃ³n.

En este caso, la arquitectura **MLP intermedia (en torno a 10âµ parÃ¡metros)** logra el **mejor balance entre bias y varianza**, alcanzando el mÃ¡ximo de accuracy tanto en validaciÃ³n como en test.

---

### ConclusiÃ³n

El experimento evidencia que **incrementar la complejidad de la red no garantiza una mejora continua del desempeÃ±o**. La efectividad del aprendizaje depende de un equilibrio entre tamaÃ±o del modelo, regularizaciÃ³n y cantidad de datos.

Este resultado refuerza la importancia del **diseÃ±o experimental y la selecciÃ³n cuidadosa de hiperparÃ¡metros**, especialmente en arquitecturas densas aplicadas a problemas visuales, donde una red convolucional (CNN) podrÃ­a aprovechar mejor la estructura espacial del dataset.

En futuros experimentos, se sugiere:

- Incorporar **Dropout** o **BatchNormalization** para estabilizar el aprendizaje en redes mÃ¡s grandes.
- Extender las Ã©pocas y aplicar **EarlyStopping** para evitar sobreajuste.
- Evaluar arquitecturas **CNN** o **hÃ­bridas** (Conv + Dense) para comparar el impacto de la inductive bias espacial frente a un MLP puramente denso.

## **ğŸ§® Actividad: Explorar experimentacion**

## ğŸ§ª GuÃ­a de experimentaciÃ³n: Arquitecturas[Â¶](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/#guia-de-experimentacion-arquitecturas)

- Capas densas (profundidad y ancho): probar 1â€“5 capas con 64â€“2048 neuronas. Doc: Dense â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense
- Activaciones: compararÂ `relu`,Â `gelu`,Â `tanh`. Doc activations â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/activationsÂ GELU â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/activations/gelu
- NormalizaciÃ³n por lotes: encender/apagarÂ `BatchNormalization`Â entre densas. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization
- RegularizaciÃ³n por Dropout: 0.0â€“0.5 en distintas posiciones. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout
- RegularizaciÃ³n L2 (weight decay clÃ¡sico): probarÂ `1e-5`,Â `5e-5`,Â `1e-4`. Doc regularizers â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2
- Inicializadores de pesos:Â `HeNormal`,Â `GlorotUniform`. Doc initializers â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/initializers
- TamaÃ±o de batch: 32, 64, 128, 256 (estabilidad vs. tiempo). Entrenamiento (`fit`) â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit

---

## âš™ï¸ GuÃ­a de experimentaciÃ³n: Optimizadores y HiperparÃ¡metros[Â¶](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/#guia-de-experimentacion-optimizadores-y-hiperparametros)

- **Adam**:Â `learning_rate`Â en {1e-2, 5e-3, 1e-3, 5e-4}; compararÂ `beta_1`,Â `beta_2`. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam
- **SGD con momentum/Nesterov**: LR en {1e-1, 5e-2, 1e-2};Â `momentum`Â 0.0â€“0.95;Â `nesterov=True/False`. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD
- **RMSprop**: variarÂ `learning_rate`,Â `rho`. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop
- **AdamW**Â (decoupled weight decay): probarÂ `weight_decay`Â en {1e-5, 1e-4}. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW
- CompilaciÃ³n del modelo (`compile`): revisar opciones y mÃ©tricas. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile

---

## â±ï¸ GuÃ­a de experimentaciÃ³n: Callbacks[Â¶](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/#guia-de-experimentacion-callbacks)

- **EarlyStopping**:Â `monitor="val_accuracy"`/`"val_loss"`,Â `patience`Â 3â€“10,Â `restore_best_weights=True`. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping
- **ReduceLROnPlateau**:Â `monitor="val_loss"`,Â `factor=0.5`,Â `patience`Â 2â€“4; observar recuperaciones. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau
- **ModelCheckpoint**: guardar â€œmejorâ€ modelo (`save_best_only=True`), porÂ `val_accuracy`Â oÂ `val_loss`. Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint
- **TensorBoard**: comparar runs, histogramas y escalas (ya incluido en tu cÃ³digo). Doc â†’Â https://www.tensorflow.org/tensorboard/get_started
- **LearningRateScheduler**: diseÃ±ar calendarios de LR (cosine, step, warmups simples). Doc â†’Â https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler

---

## ğŸ¡ Para probar en casa: otros datasets[Â¶](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/#para-probar-en-casa-otros-datasets)

**Objetivo:**Â repetir la misma receta (aplanar â†’ MLP â†’ entrenar) con distintos conjuntos de datos.

### Datasets de Keras (plug & play)[Â¶](https://juanfkurucz.com/ucu-ia/ut2/08-backpropagation-optimizadores/#datasets-de-keras-plug-play)

- **MNIST**Â (28Ã—28 gris, dÃ­gitos 0â€“9). Doc:Â https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data
- **Fashion-MNIST**Â (28Ã—28 gris, ropa 10 clases). Doc:Â https://www.tensorflow.org/api_docs/python/tf/keras/datasets/fashion_mnist/load_data
- **CIFAR-100**Â (32Ã—32Ã—3, 100 clases). Doc:Â https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar100/load_data
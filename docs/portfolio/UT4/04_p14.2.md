---
title: "Práctica 14.2"
date: 2025-01-01
---

# **Práctica 14.2: Agentes con LangGraph - RAG, Tools y Memoria Conversacional**

- [Consigna](https://juanfkurucz.com/ucu-ia/ut4/14-langchain-openai-intro/)


## **Diferencia entre usar un agente LangGraph vs. llamar al LLM directo**

La principal diferencia es que **LangGraph introduce un estado explícito que se mantiene y evoluciona a lo largo de la conversación**, mientras que una invocación directa (`llm.invoke(...)`) es completamente *stateless*.

Con LangGraph:

- El estado (AgentState) contiene los mensajes previos y cualquier metadato adicional.
- Cada nodo del grafo puede leer y modificar ese estado.
- La conversación multi-turn se modela como un flujo controlado entre nodos (assistant → tools → assistant → memory…).

Esto permite diseñar agentes **deterministas, controlables y auditables**, en contraste con el prompting libre y sin estructura.

## **El estado viajando por el grafo**

En LangGraph se ve explícitamente que:

- Cada nodo recibe el `state` como entrada.
- El nodo devuelve un nuevo estado (o parte de él) que alimenta la siguiente transición.
- Las conexiones START → assistant → tools → assistant → END muestran cómo el estado fluye a través del sistema.

La trazabilidad del estado es fundamental para control, debugging y reproducibilidad.

## **Ventaja de guardar un “summary” en vez de todo el historial**

Guardar un *summary* ofrece varias ventajas:

- Reduce costos: no es necesario reenviar todo el historial completo al LLM en cada turno, disminuyendo tokens y latencia.
- Mantiene contexto semántico suficiente sin sobresaturar el prompt.
- Escala mejor en conversaciones largas donde el historial crece indefinidamente.

### **Privacidad**

El summary no debe incluir:

- Nombres propios del usuario,
- Datos financieros,
- IDs, direcciones, pedidos o datos identificables,
- Información sensible o confidencial.

Debe contener solo la **intención** y los **acuerdos conceptuales**, no detalles personales.

## **¿Qué pasaría si el corpus RAG fuera mucho más grande?**

Con un corpus grande:

- El **costo de recuperación** aumentaría, requiriendo motores más eficientes como Chroma/Weaviate/Elasticsearch.
- Debería usarse *k* pequeño y/o filtros semánticos.
- Sería necesario un pipeline de **chunking adecuado**, normalización y deduplicación.

Además, devolver textos demasiado largos al modelo:

- incrementa tokens consumidos,
- aumenta latencia,
- diluye la señal relevante.

En producción se devuelve **solo el top-k mínimo necesario**, idealmente contenido extractado o condensado.

## **Problemas de las tools adicionales en producción**

Las tools simples pueden tener riesgos:

- `get_order_status` podría filtrar información sensible si no maneja autenticación.
- Tools matemáticas pueden ejecutar código inseguro si evalúan expresiones arbitrarias.
- Tools basadas en diccionarios pueden volverse inconsistentes sin una base de datos real.

### **Mejoras para producción**

- Validación estricta de inputs.
- Autenticación/autorización.
- Logs auditables.
- Limitar datos expuestos.
- Uso de fuentes externas confiables (APIs seguras).

## **¿Dónde ocurre el “reasoning” en el grafo?**

El razonamiento ocurre en el **assistant_node**, donde el LLM analiza:

- el estado actual,
- los mensajes previos,
- las herramientas disponibles,
- y decide si responder directamente o llamar a una tool.

El ToolNode **no razona**; solo ejecuta la tool solicitada.

El razonamiento es exclusivo del LLM, mientras que la orquestación es responsabilidad del grafo.

## **¿Cómo cambia el diseño si tuvieras 10 tools en vez de 2 o 3?**

Con muchas tools:

- Es indispensable definir descripciones claras y específicas para cada una.
- Se necesita un *router* más fino o un *tool selector* basado en embeddings o etiquetas.
- Puede ser útil organizar tools en categorías (“sistema”, “consultas”, “finanzas”).
- El prompt del asistente debe guiar explícitamente **cuándo conviene llamar cada tool**.

Esto evita comportamientos erráticos y reduce llamadas innecesarias.

## **¿Cómo reconocer cuándo el agente está llamando rag_search vs. otras tools?**

El agente llama `rag_search` cuando:

- La pregunta solicita información conceptual del corpus local.
- El mensaje del LLM contiene `tool_calls=[...]` con el nombre `"rag_search"`.

Reconocerlo es fácil revisando:

- el estado (`state["messages"][-1]`),
- o la UI (Gradio) donde se listan las herramientas utilizadas.

`get_order_status` o `get_utc_time` solo aparecerán si el usuario hace consultas explícitas que lo justifiquen.

## **¿Qué prompts dar al modelo para que use tools “con criterio”?**

Prompts recomendados:

- “Llamá tools solo cuando la información NO esté presente en el historial.”
- “Usá rag_search exclusivamente para información del corpus interno.”
- “No llames herramientas si la respuesta puede generarse directamente.”
- “Si la pregunta es de estado interno (hora, pedidos), usá la tool correspondiente.”
- “Si no hay suficiente contexto, devolvé ‘No suficiente contexto’.”

El *system prompt* debe ser explícito y normativo.

## **¿Cada cuánto actualizar el summary?**

Opciones razonables:

- Cada 2–3 turnos.
- Cada vez que se identifica un cambio significativo en la intención del usuario.
- Antes de que el historial supere un umbral de tokens.

Actualizarlo en cada turno puede generar costos innecesarios, pero actualizarlo muy tarde puede causar pérdida de contexto.

## **¿Qué excluir del summary?**

Debe excluir:

- Identificadores personales,
- Datos sensibles,
- Información confidencial del usuario,
- Números concretos que no sean semánticamente relevantes,
- Respuestas completas (solo mantener la síntesis).

El objetivo del summary es mantener **intención**, no **contenido literal**.

## **¿Cómo diseñar prompts para reducir alucinación en Q&A sin RAG?**

Reglas esenciales:

- “Respondé SOLO con el contexto dado.”
- “Si el contexto es insuficiente, devolvé: ‘No suficiente contexto’.”
- “No inventes nombres, fechas ni definiciones.”
- “Si no encontrás la respuesta exacta, indicá que falta información.”

Estas instrucciones reducen radicalmente el riesgo de alucinaciones.

## **¿Cómo exigir formato y concisión consistentemente?**

Lo más robusto:

- Plantillas con ChatPromptTemplate.
- Límites explícitos: “<= 3 oraciones”, “<= 120 tokens”.
- JSON estructurado usando Pydantic (`with_structured_output`).
- Evitar prompts libres y combinar instrucciones estrictas con LCEL.

## **Ventajas del enfoque RAG en el agente**

- Permite grounding factual de forma verificable.
- Aumenta la precisión del agente en dominios cerrados.
- Reduce alucinaciones al obligar respuestas basadas en el corpus.
- Facilita citar fragmentos o secciones exactas.
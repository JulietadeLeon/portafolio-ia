---
title: "Práctica 14"
date: 2025-01-01
---

# **Práctica 14: LLMs con LangChain (OpenAI) - Prompting, Plantillas y Salida Estructurada (ES)**

- [Consigna](https://juanfkurucz.com/ucu-ia/ut4/14-langchain-openai-intro/)
- [Google Colab](https://drive.google.com/file/d/1UndPh3mo_JVRW03k1DRunAQA90XbwkuX/view?usp=sharing)


## **Cambios al pedir 1 vs. 3 oraciones**

Cuando se solicita **una oración**, el modelo tiende a producir una respuesta más directa, sintética y enfocada en el concepto central. En cambio, cuando se piden **tres oraciones**, el modelo expande la idea, agrega contexto y ejemplos, y ofrece una explicación más matizada. Esto refleja que los LLM ajustan la densidad informativa en función de las restricciones estructurales del prompt. En entornos productivos, este control permite modular el nivel de detalle según el caso de uso.

## **Variancia entre ejecuciones con la misma consigna**

Aun con *temperature = 0*, puede observarse una variabilidad mínima en el ordenamiento o estilo, debido a detalles internos del decodificador. Con *temperature > 0*, la variancia aumenta claramente, generando diferencias en vocabulario, estructura y creatividad. Esto refuerza la idea de que tareas evaluables o deterministas deben ejecutarse con *temperature = 0* para garantizar estabilidad y reproducibilidad.

## **Claridad vs. creatividad según parámetros (temperature, max_tokens, top_p)**

- **Temperature baja (0–0.3):** promueve claridad, consistencia y respuestas más deterministas. Ideal para tareas cerradas: clasificación, extracción, traducción o instrucciones de software.
- **Temperature media (0.5):** equilibra creatividad y control, útil para resúmenes o generación de textos no estrictos.
- **Temperature alta (0.9):** genera mayor variabilidad y exploración léxica, pero menor precisión factual.

En general, **claridad → temperature baja + top_p moderado**, mientras que **creatividad → temperature y top_p más altos**.

Los parámetros impactan fuertemente en tareas cerradas: una configuración demasiado creativa puede introducir ruido, ambigüedad o directamente errores factuales.

## **Few-shot vs. Zero-shot y efecto del template**

El enfoque **zero-shot** funciona bien para tareas simples y definiciones breves, pero puede resultar inconsistente en tareas donde la estructura o el estilo son importantes. El **few-shot** mejora significativamente la precisión del formato y la estabilidad de las etiquetas, ya que proporciona ejemplos explícitos del comportamiento esperado.

Cuando el template fija estructura (por ejemplo, “3 bullets, tono conciso”), la salida es más uniforme y menos propensa a desviaciones, lo cual es esencial para cadenas de producción con validación automatizada.

## **Ventajas de structured output vs. “parsear a mano”**

El uso de **with_structured_output()** evita la fragilidad típica de parsear cadenas JSON generadas por prompts ad hoc. El modelo devuelve directamente objetos validados (Pydantic), lo cual garantiza:

- cumplimiento del esquema,
- tipos correctos,
- ausencia de JSON inválido,
- robustez ante cambios mínimos del texto.

En producción, esto es crítico para asegurar **contratos formales de salida**, especialmente en sistemas que alimentan bases de datos, APIs o pipelines downstream.

## **Prompt que consume más tokens y balance latencia/calidad**

Los prompts más largos, con ejemplos few-shot o instrucciones detalladas, incrementan el consumo de tokens. En particular, prompts con contexto extenso o múltiples secciones duplican fácilmente la latencia.

El balance se logra optimizando:

- **compactación del prompt**,
- **instrucciones claras**,
- **solo incluir la mínima cantidad de ejemplos necesarios**,
- reutilizar plantillas en lugar de construir prompts ad hoc.
    
    Latencia baja exige prompts breves; calidad alta requiere más contexto. La decisión depende del SLA del sistema.
    

## **Cuándo alucina el modelo ante falta de contexto**

El modelo tiende a alucinar cuando el contexto es insuficiente o ambiguo, especialmente en Q&A sin RAG. Aunque el prompt indique “respondé SOLO con el contexto”, los LLM pueden completar lagunas mediante inferencias no justificadas.

Para mitigar esto:

- incluir instrucciones estrictas,
- definir reglas de fallback (“No suficiente contexto”),
- combinar modelo + RAG en flujos críticos.

## **Cómo exigir formato y concisión de manera consistente**

Se logra mediante:

- **plantillas estructuradas**,
- restricciones explícitas (p. ej., “<=120 tokens”),
- enumeraciones forzadas (Intro / Hallazgos / Recomendación),
- JSON validado con Pydantic.

Los modelos siguen mucho mejor información que viene desde el *system prompt* y combinada con LCEL, en vez de depender solamente del texto del usuario.

## **Efecto de zero-shot vs few-shot**

El **few-shot** mejora claramente la consistencia y la precisión de etiquetas, especialmente para clasificación de sentimientos o tareas con ambigüedad semántica. Proporcionar 1–2 ejemplos reduce la variancia y estandariza el tono.

Por otro lado, **zero-shot** es más flexible y simple, pero se vuelve más impredecible cuando el modelo no infiere correctamente el formato o la granularidad requerida.

## **Resumen directo vs. map-reduce (multi-document)**

El resumen directo funciona bien con textos moderados, pero pierde información o genera omisiones en textos extensos.

El enfoque **map-reduce** con *chunking* ofrece:

- mejor cobertura del contenido,
- menos riesgo de omitir conceptos,
- más control sobre redundancia.
    
    El *chunk_size* y *chunk_overlap* determinan el equilibrio entre coherencia global y granularidad: chunks muy pequeños pierden contexto; chunks muy grandes generan latencia alta y riesgo de corte de ideas.
    

## **Robustez en extracción de información estructurada**

Los modelos tienden a fallar en:

- fechas ambiguas (“05/11” sin año),
- nombres propios poco comunes,
- asociaciones incorrectas entre entidades (ORG/LOC).

Definir campos estrictos, validar tipos y usar `Literal` para categorías reduce drásticamente los errores. En producción, la extracción estructurada exige esquemas estrictos.

## **Cuándo conviene usar RAG vs. no usar RAG**

Sin RAG, el modelo depende totalmente del prompt y tiende a completar información ausente, lo cual introduce riesgo de alucinación.

RAG conviene cuando:

- el dominio es dinámico (FAQs, políticas, guías),
- existe corpus local,
- se necesitan respuestas grounded y citables.

Para preguntas de conocimiento general, RAG no aporta grandes mejoras; para documentos o FAQ específicas, es imprescindible.
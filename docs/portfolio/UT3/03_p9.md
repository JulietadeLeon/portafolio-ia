---
title: "Práctica 9"
date: 2025-01-01
---

# **Práctica 9: CNNs y Transfer Learning con TensorFlow/Keras**

- [Consigna](https://juanfkurucz.com/ucu-ia/ut3/09-cnn-transfer-learning-assignment/)
- [Google Colab](https://colab.research.google.com/drive/1kBk03G8fboN7U16EvUXJG3X1884wisaX?usp=sharing)

## Implementación de una Red Neuronal Convolucional Básica desde Cero

En esta etapa se implementó una **Red Neuronal Convolucional (CNN)** desde cero, con el objetivo de comprender la estructura fundamental de este tipo de arquitecturas y su aplicación al conjunto de datos **CIFAR-10**, que contiene imágenes a color de 32x32 píxeles clasificadas en 10 categorías.

El modelo fue diseñado utilizando la API **Sequential de Keras**, organizando sus componentes en bloques jerárquicos de convolución, activación y *pooling*. La estructura final del modelo se compone de los siguientes elementos:

- **Bloque Convolucional 1:** una capa `Conv2D` con 32 filtros de tamaño 3x3, seguida de una activación **ReLU** y una capa de **MaxPooling2D** (2x2), que reduce la dimensionalidad y conserva las características más relevantes.
- **Bloque Convolucional 2:** una segunda capa `Conv2D` con 64 filtros y la misma configuración de activación y *pooling*, permitiendo extraer patrones más complejos a partir de las representaciones anteriores.
- **Clasificador Final:** las salidas de las capas convolucionales se aplanan mediante `Flatten()` y se conectan a una capa densa de 512 neuronas con activación **ReLU**, seguida de una capa de salida `Dense(10, activation='softmax')`, responsable de asignar probabilidades a cada una de las diez clases del dataset.

El modelo fue compilado utilizando el **optimizador Adam** con una tasa de aprendizaje de 0.001 y la función de pérdida **categorical_crossentropy**, apropiada para problemas de clasificación multiclase.

La arquitectura resultante mostró el siguiente resumen:

| Tipo de Capa | Salida | Parámetros |
| --- | --- | --- |
| Conv2D (32 filtros, 3x3) | (32, 32, 32) | 896 |
| MaxPooling2D | (16, 16, 32) | 0 |
| Conv2D (64 filtros, 3x3) | (16, 16, 64) | 18,496 |
| MaxPooling2D | (8, 8, 64) | 0 |
| Flatten | (4096) | 0 |
| Dense (512) | (512) | 2,097,664 |
| Dense (10 – Softmax) | (10) | 5,130 |
| **Total de Parámetros Entrenables:** |  | **2,122,186** |

El modelo alcanza un total de **2.12 millones de parámetros entrenables**, lo que demuestra la complejidad y capacidad de representación que incluso una arquitectura simple puede alcanzar. Este número refleja el balance entre **profundidad y capacidad de generalización**, siendo suficiente para capturar patrones visuales en imágenes pequeñas sin riesgo inmediato de sobreajuste.

En términos conceptuales, esta práctica permitió afianzar los fundamentos del diseño de redes convolucionales: la importancia de las capas de convolución para la detección de características locales, el rol del *pooling* en la reducción de dimensionalidad y el uso de capas densas como etapa de clasificación final.

Asimismo, proporcionó una comprensión práctica sobre cómo la elección del número de filtros, el tamaño de los *kernels* y la función de activación inciden directamente en la capacidad del modelo para aprender representaciones jerárquicas de los datos visuales.

## **Análisis Comparativo: CNN Simple vs. Transfer Learning**

![.](../../assets/image_UT3_p9.png)

![.](../../assets/image_UT3_p9_1.png)

![.](../../assets/image_UT3_p9_2.png)

### **1. Desempeño general**

Los resultados evidencian una diferencia considerable en el rendimiento entre ambas arquitecturas.

La **CNN Simple** alcanzó una **precisión final de validación del 69%**, mientras que el modelo de **Transfer Learning** obtuvo apenas **31.3%**.

Este contraste sugiere que, bajo las condiciones de entrenamiento utilizadas (dataset CIFAR-10 con imágenes de 32×32 y pocas épocas), el modelo convolucional diseñado desde cero se adaptó mejor a la tarea específica, mientras que el modelo preentrenado no logró transferir efectivamente su conocimiento.

Una posible explicación radica en la **incompatibilidad de escala y dominio** entre el dataset **ImageNet** —utilizado para el preentrenamiento del modelo base— y **CIFAR-10**, que contiene imágenes de menor resolución y distinto nivel de detalle. En consecuencia, las características aprendidas por el modelo base no resultaron directamente útiles sin un proceso adicional de *fine-tuning* o ajuste de capas superiores.

---

### **2. Pérdida y comportamiento del entrenamiento**

El análisis de las curvas de pérdida revela comportamientos opuestos entre ambos modelos:

- **CNN Simple:** la pérdida de entrenamiento disminuye de forma constante, mientras que la de validación se estanca a partir de la época 5, mostrando posteriormente un leve incremento.
    
    Esto indica la presencia de **sobreajuste (overfitting)**, dado que el modelo sigue mejorando su rendimiento sobre los datos de entrenamiento pero no logra generalizar a los datos de validación.
    
- **Transfer Learning:** tanto la pérdida de entrenamiento como la de validación presentan una tendencia descendente y paralela, con una brecha mínima entre ambas curvas.
    
    Si bien esto refleja una **buena estabilidad** y ausencia de sobreajuste, también evidencia **subajuste (underfitting)**, ya que el modelo no alcanza niveles de precisión adecuados en ninguno de los conjuntos.
    

---

### **3. Análisis de Overfitting**

El cálculo del **gap entre precisión de entrenamiento y validación** confirma lo anterior:

| Modelo | Gap (Train - Val) | Interpretación |
| --- | --- | --- |
| CNN Simple | 0.196 | Sobreajuste significativo |
| Transfer Learning | 0.010 | Mínimo gap, pero bajo rendimiento global |

El **gap de 0.196** en la CNN Simple refleja una diferencia notoria entre lo que el modelo aprende en entrenamiento y su capacidad de generalización. Por el contrario, el **gap casi nulo (0.010)** en Transfer Learning indica que el modelo no logra aprender representaciones suficientemente discriminativas, manteniéndose en un régimen de *underfitting*.

---

### **4. Reporte de Clasificación**

El análisis por clase refuerza estas observaciones.

### **CNN Simple**

- Presenta valores de **precisión (precision)** y **recuperación (recall)** equilibrados en la mayoría de las clases, con un **promedio ponderado de F1-score ≈ 0.69**.
- Las clases con mejor desempeño son *automobile (0.80 F1)*, *ship (0.81 F1)* y *airplane (0.75 F1)*, lo cual indica que el modelo logra captar bien las formas estructuradas y colores homogéneos.
- Las clases con menor rendimiento son *bird (0.57 F1)* y *dog (0.61 F1)*, probablemente debido a su mayor variabilidad visual y similitud con otras categorías.

### **Transfer Learning**

- Exhibe un rendimiento global considerablemente inferior, con un **F1-score promedio de 0.30** y **accuracy del 31%**.
- Ninguna clase supera un F1-score de 0.37, lo cual confirma que el modelo no logró adaptar las representaciones de ImageNet al dominio reducido de CIFAR-10.
- Las confusiones frecuentes entre clases sugieren que el modelo no llegó a aprender características visuales discriminativas relevantes para el conjunto.

---

### **5. Interpretación final**

En síntesis, los resultados reflejan que:

- La **CNN Simple** logró un mejor equilibrio entre aprendizaje y desempeño general, aunque con un grado notable de sobreajuste.
- El modelo de **Transfer Learning**, si bien mostró un entrenamiento más controlado y estable, no alcanzó niveles adecuados de precisión por falta de ajuste de las capas preentrenadas.
- Estos hallazgos sugieren que, en contextos con imágenes pequeñas o de dominio diferente al dataset de preentrenamiento, es necesario aplicar **estrategias de fine-tuning** (descongelar parcialmente capas superiores) o **ajustes de arquitectura** (mayor resolución de entrada, normalización previa, *global average pooling*) para aprovechar los beneficios del *transfer learning*.

---

### **6. Conclusiones**

- La **CNN simple** resultó más efectiva para CIFAR-10 en esta configuración inicial, alcanzando una precisión cercana al 70%, aunque a costa de una menor capacidad de generalización.
- El **modelo preentrenado** requiere una reconfiguración más profunda para adecuarse al dominio de imágenes pequeñas, evidenciando que el *transfer learning* no siempre garantiza mejoras sin un *fine-tuning* adecuado.
- El análisis permitió comprender cómo las diferencias en tamaño de imagen, arquitectura y estrategia de entrenamiento influyen directamente en la dinámica de pérdida, precisión y generalización de los modelos de visión artificial.

## **Investigación Libre – Posibles Líneas de Exploración**

En esta última instancia se propone un espacio de **investigación libre**, orientado a reflexionar sobre **distintas estrategias de mejora y optimización de modelos de visión por computadora**, más allá de las arquitecturas básicas implementadas previamente.

El propósito es **explorar conceptualmente** cómo distintas decisiones de diseño podrían influir en el rendimiento de los modelos, sin necesidad de llevarlas a la práctica computacional.

A continuación se detallan **posibles líneas de investigación y análisis** que podrían abordarse en esta etapa:

### **1. Variación de arquitecturas**

Se podría estudiar el impacto de modificar la **profundidad, el número de filtros o la estructura de los bloques convolucionales**.

Por ejemplo, comparar una arquitectura simple (dos capas convolucionales y una densa final) con una red más profunda de tipo **VGG**, o incorporar **bloques residuales** inspirados en **ResNet** para analizar si la presencia de conexiones de salto mejora la estabilidad del entrenamiento y la capacidad de generalización.

Asimismo, sería interesante examinar **arquitecturas eficientes**, como **MobileNetV2** o **EfficientNet**, que equilibran desempeño y costo computacional.

---

### **2. Comparación de modelos base para Transfer Learning**

Otra línea de exploración consiste en analizar **diferentes modelos preentrenados** disponibles en `keras.applications`, evaluando teóricamente qué ventajas ofrecería cada uno.

Por ejemplo:

- **ResNet50:** alta capacidad representacional y robustez.
- **MobileNetV2:** eficiencia y menor cantidad de parámetros.
- **EfficientNetB0:** escalabilidad y equilibrio entre precisión y eficiencia.

El análisis podría centrarse en cómo las características extraídas por estos modelos afectan la precisión en conjuntos de datos pequeños, como CIFAR-10, y cuándo resulta conveniente aplicar **fine-tuning** en lugar de utilizar solo el modelo como extractor de características.

---

### **3. Incorporación de técnicas de regularización**

Una tercera posibilidad sería explorar el efecto conceptual de agregar **capas de Batch Normalization o Dropout** dentro de la arquitectura.

Estas técnicas ayudan a **reducir el sobreajuste (overfitting)** y a **mejorar la estabilidad del entrenamiento**, ya sea normalizando la activación de las capas o introduciendo ruido controlado durante el aprendizaje.

La reflexión podría incluir cuándo resulta recomendable aplicar cada una, y cómo afectan al equilibrio entre precisión y generalización del modelo.

---

### **4. Habilitar el Fine-Tuning**

También podría plantearse la comparación entre **feature extraction** y **fine-tuning parcial o total** del modelo base.

Teóricamente, el *feature extraction* aprovecha las representaciones generales aprendidas por un modelo preentrenado, mientras que el *fine-tuning* ajusta parte de sus capas para adaptarlas a un dominio específico.

Analizar bajo qué condiciones conviene cada enfoque —por ejemplo, según la cantidad de datos disponibles o la similitud entre el dataset original y el nuevo— constituiría una línea de investigación valiosa.

---

### **5. Ajuste de hiperparámetros**

Finalmente, se podrían considerar variaciones conceptuales en los **hiperparámetros** de entrenamiento:

- **Learning rate:** tasas más altas o bajas modifican la velocidad y estabilidad del aprendizaje.
- **Batch size:** influye en la estimación del gradiente y en la convergencia.
- **Optimizadores:** comparar Adam, SGD o RMSprop desde un punto de vista teórico, analizando sus diferencias en actualización de pesos y sensibilidad al gradiente.

Este tipo de análisis permitiría comprender **cómo los parámetros de configuración inciden directamente en el comportamiento del modelo**, incluso sin necesidad de ejecutar código.

---

### **Reflexión final**

En síntesis, esta instancia invita a **pensar como investigadora en aprendizaje automático**, identificando variables, formulando hipótesis y anticipando resultados posibles.

Más allá de la implementación práctica, el ejercicio de reflexión sobre **arquitecturas, regularización, transferencia y ajuste de parámetros** permite desarrollar criterio técnico, comprensión crítica y autonomía para el diseño de futuros modelos.
---
title: "PrÃ¡ctica 6"
date: 2025-01-01
---

# **PrÃ¡ctica 6: Clustering y PCA - Mall Customer Segmentation**

- [Consigna](https://juanfkurucz.com/ucu-ia/ut1/06-clustering-pca/)
- [Datos](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python/data)
- [Google Colab](https://colab.research.google.com/drive/1RyvHVtyCCUGWQB-bS1vmAhGed_e-b4RW?usp=sharing)

# **FASE 1: BUSINESS UNDERSTANDING**

*"Â¿QuÃ© problema estamos resolviendo?"*

## **CONTEXTO DEL ANÃLISIS**

**PROBLEMA DE SEGMENTACIÃ“N:**

Los centros comerciales necesitan entender mejor a sus clientes para optimizar sus estrategias de marketing. El objetivo es segmentar clientes basÃ¡ndose en: - PersonalizaciÃ³n de campaÃ±as de marketing

- Ofertas especÃ­ficas por tipo de cliente - OptimizaciÃ³n de inversiÃ³n publicitaria - ComprensiÃ³n de patrones de comportamiento de compra

**DATASET DE TRABAJO:**Â Utilizaremos elÂ **Mall Customer Segmentation Dataset**Â que contiene informaciÃ³n demogrÃ¡fica y comportamental real de clientes de centros comerciales:

- **Fuente:**Â [Mall Customer Segmentation Dataset - Kaggle](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python/data)
- **Registros:**Â ~200 clientes con informaciÃ³n completa
- **Variables:**Â CustomerID, Genre, Age, Annual Income, Spending Score
- **Ventaja:**Â Datos limpios y listos para anÃ¡lisis, perfectos para demostrar tÃ©cnicas de clustering

# **FASE 2: DATA UNDERSTANDING**

*"Â¿QuÃ© nos dicen los datos?"*

## **Paso 2.1: Setup Inicial**

**Pistas:**

- [DocumentaciÃ³n Pandas](https://pandas.pydata.org/docs/)Â - Biblioteca para manipulaciÃ³n y anÃ¡lisis de datos
- [DocumentaciÃ³n NumPy](https://numpy.org/doc/stable/)Â - Biblioteca para computaciÃ³n numÃ©rica con arrays
- Â¿QuÃ© biblioteca usas para DataFrames y Series?
- Â¿QuÃ© biblioteca proporciona arrays multidimensionales y funciones matemÃ¡ticas?

## **Paso 2.2: Carga del Dataset**

## **Paso 2.3: InspecciÃ³n Inicial del Dataset**

![.](../../assets/image_UT1_p6_1.png)

## **Paso 2.4: AnÃ¡lisis de Tipos de Datos**

![.](../../assets/image_UT1_p6_2.png)

## **Paso 2.5: AnÃ¡lisis de DistribuciÃ³n por GÃ©nero**

![.](../../assets/image_UT1_p6_3.png)

## **Paso 2.6: EstadÃ­sticas de Variables Clave**

![.](../../assets/image_UT1_p6_4.png)
 
## **Paso 2.7: DetecciÃ³n de Outliers**

![.](../../assets/image_UT1_p6_5.png)
 
## **Paso 2.8: Visualizaciones - Distribuciones**

**Pistas:**

- [DocumentaciÃ³n Matplotlib](https://matplotlib.org/stable/contents.html)Â - Biblioteca principal para visualizaciÃ³n en Python
- [DocumentaciÃ³n Seaborn](https://seaborn.pydata.org/)Â - Biblioteca de visualizaciÃ³n estadÃ­stica basada en matplotlib
- Â¿QuÃ© biblioteca proporcionaÂ `pyplot`Â para grÃ¡ficos bÃ¡sicos?
- Â¿QuÃ© biblioteca ofrece paletas de colores y estilos mejorados para grÃ¡ficos estadÃ­sticos?

![.](../../assets/image_UT1_p6_6.png)
 
## **Paso 2.9: Visualizaciones - Relaciones**

![.](../../assets/image_UT1_p6_7.png)
 
## **Paso 2.10: Matriz de CorrelaciÃ³n**

![.](../../assets/image_UT1_p6_8.png)
 
![.](../../assets/image_UT1_p6_9.png)
 
## **Paso 2.11: AnÃ¡lisis Comparativo por GÃ©nero**

![.](../../assets/image_UT1_p6_10.png)
 
## **Paso 2.12: SÃ­ntesis de Insights**

INSIGHTS PRELIMINARES - COMPLETE:

COMPLETE BASÃNDOTE EN TUS OBSERVACIONES:
Variable con mayor variabilidad: Annual Income (k$)
Â¿Existe correlaciÃ³n fuerte entre alguna variable? No, las correlaciones son dÃ©biles
Â¿QuÃ© variable tiene mÃ¡s outliers? Annual Income (k$)
Â¿Los hombres y mujeres tienen patrones diferentes? SÃ­, en Spending Score se ven diferencias en la dispersiÃ³n
Â¿QuÃ© insight es mÃ¡s relevante para el anÃ¡lisis? Los clientes se agrupan naturalmente por ingreso y score de gasto
Â¿QuÃ© 2 variables serÃ¡n mÃ¡s importantes para clustering? Annual Income (k$) y Spending Score (1-100)

PREPARÃNDOSE PARA CLUSTERING:
Â¿QuÃ© relaciÃ³n entre Income y Spending Score observas? Se forman grupos visibles de clientes con bajo/alto ingreso y bajo/alto gasto
Â¿Puedes imaginar grupos naturales de clientes? SÃ­, al menos 4-5 segmentos que diferencian niveles de ingreso y score de gasto

## **Paso 2.13: IdentificaciÃ³n de Features para Clustering**

![.](../../assets/image_UT1_p6_11.png)
 
## **Paso 2.14: CodificaciÃ³n de Variables CategÃ³ricas con OneHotEncoder**

**Pistas:**

- [DocumentaciÃ³n OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)Â - Codificador para variables categÃ³ricas
- [DocumentaciÃ³n fit_transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.fit_transform)Â - MÃ©todo para ajustar y transformar en un paso
- [GuÃ­a sobre variables dummy](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)Â - CÃ³mo manejar variables categÃ³ricas
- [Diferencia drop='first' vs completo](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)Â - CuÃ¡ndo usar cada enfoque

![.](../../assets/image_UT1_p6_12.png)
 
## **Paso 2.15: PreparaciÃ³n del Dataset Final**

![.](../../assets/image_UT1_p6_12.1.png)
 
## **Paso 2.16: VerificaciÃ³n de Calidad de Datos**

![.](../../assets/image_UT1_p6_13.png)
 
## **Paso 2.17: AnÃ¡lisis de Escalas (Pre-NormalizaciÃ³n)**

ANÃLISIS DE ESCALAS - Â¿Por quÃ© necesitamos normalizaciÃ³n?

## ESTADÃSTICAS POR VARIABLE

**Age**  
- Rango: 18.0 â€“ 70.0  
- Media: 38.9  
- DesviaciÃ³n: 14.0  

**Annual Income (k$)**  
- Rango: 15.0 â€“ 137.0  
- Media: 60.6  
- DesviaciÃ³n: 26.3  

**Spending Score (1â€“100)**  
- Rango: 1.0 â€“ 99.0  
- Media: 50.2  
- DesviaciÃ³n: 25.8  

**Genre_Female**  
- Rango: 0.0 â€“ 1.0  
- Media: 0.6  
- DesviaciÃ³n: 0.5  

**Genre_Male**  
- Rango: 0.0 â€“ 1.0  
- Media: 0.4  
- DesviaciÃ³n: 0.5  

---

## ANÃLISIS DE LAS ESTADÃSTICAS â€“ COMPLETA

- **Â¿QuÃ© variable tiene el rango mÃ¡s amplio?**  
  Annual Income (k$)  

- **Â¿CuÃ¡l es la distribuciÃ³n de gÃ©nero en el dataset?**  
  Balanceada (hombres y mujeres en proporciones similares)  

- **Â¿QuÃ© variable muestra mayor variabilidad (std)?**  
  Annual Income (k$)  

- **Â¿Los clientes son jÃ³venes o mayores en promedio?**  
  JÃ³venesâ€“adultos (promedio ~38 aÃ±os)  

- **Â¿El income promedio sugiere quÃ© clase social?**  
  Clase media â€“ media alta  

- **Â¿Por quÃ© la normalizaciÃ³n serÃ¡ crÃ­tica acÃ¡?**  
  Porque las escalas de Age, Income y Spending son muy diferentes y afectarÃ­an el clustering  


LISTO PARA DATA PREPARATION con 5 features

# **FASE 3: DATA PREPARATION**

*"Preparando los datos para el modelado"*

## **Paso 3.1: Setup para NormalizaciÃ³n**

![.](../../assets/image_UT1_p6_14.png)
 
## **Paso 3.2: Aplicar los 3 Scalers**

**Pistas:**

- [DocumentaciÃ³n MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)Â - Escalado a rango especÃ­fico (default [0,1])
- [DocumentaciÃ³n StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)Â - EstandarizaciÃ³n con media=0 y std=1
- [DocumentaciÃ³n RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)Â - Escalado robusto usando mediana e IQR
- [GuÃ­a de preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)Â - CuÃ¡ndo usar cada scaler
- Â¿CuÃ¡l es el mÃ©todo que ajusta y transforma los datos en un solo paso?

![.](../../assets/image_UT1_p6_15.png)
 
## **Paso 3.3: ComparaciÃ³n Visual - Boxplots**

![.](../../assets/image_UT1_p6_16.png)
 
## **Paso 3.4: ComparaciÃ³n de Distribuciones**

![.](../../assets/image_UT1_p6_17.png)
 
## **Paso 3.5: AnÃ¡lisis EstadÃ­stico Post-Scaling**

![.](../../assets/image_UT1_p6_18.png)
 
## **Paso 3.6: Test de Impacto en Clustering**

**Pistas:**

- [DocumentaciÃ³n KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)Â - Algoritmo de clustering k-means
- [DocumentaciÃ³n fit_predict](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit_predict)Â - MÃ©todo para entrenar y predecir clusters
- [DocumentaciÃ³n silhouette_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)Â - MÃ©trica de calidad de clustering [-1, 1]
- [GuÃ­a de clustering](https://scikit-learn.org/stable/modules/clustering.html#k-means)Â - Conceptos bÃ¡sicos de K-Means
- âš ï¸Â **IMPORTANTE**:Â `fit_predict()`Â devuelve etiquetas 1D,Â `fit_transform()`Â devuelve distancias 2D
- Â¿CuÃ¡l mÃ©todo necesitas para obtener las etiquetas de cluster (no las distancias)?
- Â¿QuÃ© significa un silhouette score mÃ¡s alto vs mÃ¡s bajo?

![.](../../assets/image_UT1_p6_19.png)
 
## **Paso 3.7: DecisiÃ³n Final de Scaler**

DECISIÃ“N FINAL DEL SCALER:

COMPLETE TU ANÃLISIS:
Mejor scaler segÃºn silhouette: MinMax
Â¿Por quÃ© crees que funcionÃ³ mejor? Porque ajustÃ³ mejor las diferencias de escala entre Income y Spending, evitando que una variable domine el clustering
Â¿AlgÃºn scaler tuvo problemas obvios? SÃ­, MinMax puede ser muy sensible a outliers y distorsionar los resultados

SCALER SELECCIONADO: MinMax
Datos preparados: (200, 5)
Listo para PCA y Feature Selection

## **Paso 3.8: PCA - ReducciÃ³n de Dimensionalidad**

**Pistas:**

- [DocumentaciÃ³n PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)Â - AnÃ¡lisis de Componentes Principales
- [DocumentaciÃ³n fit_transform](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform)Â - MÃ©todo para ajustar y transformar con PCA
- [GuÃ­a de reducciÃ³n de dimensionalidad](https://scikit-learn.org/stable/modules/decomposition.html#pca)Â - Conceptos y casos de uso de PCA
- [InterpretaciÃ³n de componentes](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)Â - CÃ³mo entender las componentes principales
- Â¿QuÃ© mÃ©todo ajusta el PCA y transforma los datos en un solo paso?

![.](../../assets/image_UT1_p6_20.png)
 
```
ğŸ¯ DECISIÃ“N DE COMPONENTES:
   ğŸ“Š Para retener 90% varianza: 3 componentes
   ğŸ“Š Para retener 95% varianza: 4 componentes
   ğŸ¯ Para visualizaciÃ³n: 2 componentes (86.3% varianza)

PCA aplicado:
   ğŸ“Š Dimensiones: (200, 5) â†’ (200, 2)
   ğŸ“ˆ Varianza explicada: 86.3%

ğŸ” INTERPRETACIÃ“N DE COMPONENTES:

   PC1 (varianza: 72.6%):
                 Age:   0.029 â†‘
     Annual Income (k$):   0.019 â†‘
     Spending Score (1-100):  -0.027 â†“
        Genre_Female:  -0.706 â†“
          Genre_Male:   0.706 â†‘

   PC2 (varianza: 13.7%):
                 Age:   0.727 â†‘
     Annual Income (k$):  -0.026 â†“
     Spending Score (1-100):  -0.685 â†“
        Genre_Female:   0.027 â†‘
          Genre_Male:  -0.027 â†“
```

![.](../../assets/image_UT1_p6_21.png)
 
ğŸ’¡ INTERPRETACIÃ“N DE NEGOCIO:
ğŸ¯ PC1 parece representar: combinaciÃ³n de ingreso anual y spending (capacidad y propensiÃ³n de compra)
ğŸ¯ PC2 parece representar: un eje etario/sexo que matiza el patrÃ³n de gasto
ğŸ“Š Los clusters visibles sugieren: segmentos alto ingresoâ€“alto gasto, alto ingresoâ€“bajo gasto, bajo ingresoâ€“alto gasto y bajo ingresoâ€“bajo gasto

## **ğŸ”Â Paso 3.9: Feature Selection - Alternativas a PCA**

### **Paso 1: Imports y Setup Feature Selection**

**Pistas:**

- [DocumentaciÃ³n SequentialFeatureSelector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)Â - Para Forward/Backward Selection

### **Paso 2: Setup y FunciÃ³n de EvaluaciÃ³n**

**Pistas para Estimador Personalizado:**

- [DocumentaciÃ³n BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)Â - ProporcionaÂ `get_params()`Â yÂ `set_params()`
- [DocumentaciÃ³n ClassifierMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html)Â - ProporcionaÂ `_more_tags()`Â para clasificadores
- **Interfaz Requerida**:Â `fit(X, y=None)`,Â `score(X, y=None)`,Â `predict(X)`
- **Clonabilidad**:Â `BaseEstimator`Â hace que el objeto sea clonable para cross-validation
- Â¿QuÃ© dos clases base necesitas importar para crear un estimador sklearn compatible?

![.](../../assets/image_UT1_p6_22.png)
 
### **Paso 3: Baseline - Todas las Features**

![.](../../assets/image_UT1_p6_23.png)
 
### **Paso 4: Forward Selection**

**Pistas:**

- Â¿QuÃ© direcciÃ³n usa Forward Selection: 'forward' o 'backward'?
- Â¿QuÃ© mÃ©todo entrena el selector:Â `fit()`Â oÂ `transform()`?
- Â¿QuÃ© mÃ©todo obtiene la mÃ¡scara de features seleccionadas:Â `get_support()`Â oÂ `support_`?
- **Importante**: SequentialFeatureSelector requiere un estimador con mÃ©todosÂ `fit()`Â yÂ `score()`

![.](../../assets/image_UT1_p6_24.png)
 
### **Paso 5: Backward Elimination**

**Pistas:**

- Â¿QuÃ© direcciÃ³n usa Backward Elimination: 'forward' o 'backward'?
- Â¿En quÃ© se diferencia conceptualmente Forward de Backward?

![.](../../assets/image_UT1_p6_25.png)
 
### **Paso 6: ComparaciÃ³n Final**

![.](../../assets/image_UT1_p6_26.png)
 
### **Paso 7: VisualizaciÃ³n Comparativa**

![.](../../assets/image_UT1_p6_27.png)
 
### **Paso 8: AnÃ¡lisis y DecisiÃ³n Final**

![.](../../assets/image_UT1_p6_28.png)
 
### **Paso 9: DecisiÃ³n para el Pipeline Final**

![.](../../assets/image_UT1_p6_29.png)
 
# **ğŸ¤–Â FASE 4: MODELING**

*"Creando los segmentos de clientes"*

## **ğŸ§©Â Paso 4.1: K-Means Clustering - Encontrando los Grupos**

![.](../../assets/image_UT1_p6_30.png)
 
ğŸ§  ELBOW METHOD - DEEP DIVE ANALYSIS:

ğŸ“‰ **Â¿QuÃ© es exactamente 'el codo'?**

- **MatemÃ¡ticamente:** Punto donde la segunda derivada de WCSS vs K cambia mÃ¡s dramÃ¡ticamente
- **Visualmente:** Donde la curva pasa de 'caÃ­da empinada' a 'caÃ­da suave'
- **Conceptualmente:** Balance entre simplicidad (menos clusters) y precisiÃ³n (menor error)

ğŸ“Š **AnÃ¡lisis cuantitativo del codo:**
K=2: Î” Inertia=-7.68, Î”Â²=0.53
K=3: Î” Inertia=-7.15, Î”Â²=6.16
K=4: Î” Inertia=-1.00, Î”Â²=0.11
K=5: Î” Inertia=-0.89, Î”Â²=0.43
K=6: Î” Inertia=-0.46, Î”Â²=0.17

ğŸ¯ **Candidato por Elbow Method:** K=6
ğŸ¯ **Candidato por Silhouette:** K=2 (score=0.762)

ğŸ¤ **DECISIÃ“N FINAL:**
âš–ï¸  Elbow sugiere K=6, Silhouette sugiere K=2
ğŸ’¼ Considerando el contexto de negocio (3-5 segmentos esperados)...
Elegimos K = 4 (balance elbow + contexto negocio)

ğŸ¯ ENTRENANDO MODELO FINAL CON K=4
Modelo entrenado:
ğŸ“Š Silhouette Score: 0.686
ğŸ¯ Clusters encontrados: 4
ğŸ“ˆ Inertia final: 3.78

ğŸ‘¥ DISTRIBUCIÃ“N DE CLIENTES:
Cluster 0: 57 clientes (28.5%)
Cluster 1: 47 clientes (23.5%)
Cluster 2: 55 clientes (27.5%)
Cluster 3: 41 clientes (20.5%)

Clusters asignados al dataset original

# **ğŸ“ˆÂ FASE 5: EVALUATION**

*"Â¿QuÃ© tan buenos son nuestros segmentos?"*

## **ğŸ“ŠÂ Paso 5.1: AnÃ¡lisis de Clusters y Perfiles**

ANALISIS DE SEGMENTOS DE CLIENTES - REPORTE EJECUTIVO

PERFILES DETALLADOS POR CLUSTER:

**CLUSTER 0** (57 clientes, 28.5%)
**Perfil DemogrÃ¡fico:**
Edad promedio: 28.4 aÃ±os
DistribuciÃ³n gÃ©nero: {'Female': np.int64(57)}
**Perfil Financiero:**
Ingreso anual: $59.7k
Spending Score: 67.7/100

**CLUSTER 1** (47 clientes, 23.5%)
**Perfil DemogrÃ¡fico:**
Edad promedio: 50.1 aÃ±os
DistribuciÃ³n gÃ©nero: {'Male': np.int64(47)}
**Perfil Financiero:**
Ingreso anual: $62.2k
Spending Score: 29.6/100

**CLUSTER 2** (55 clientes, 27.5%)
**Perfil DemogrÃ¡fico:**
Edad promedio: 48.1 aÃ±os
DistribuciÃ³n gÃ©nero: {'Female': np.int64(55)}
**Perfil Financiero:**
Ingreso anual: $58.8k
Spending Score: 34.8/100

**CLUSTER 3** (41 clientes, 20.5%)
**Perfil DemogrÃ¡fico:**
Edad promedio: 28.0 aÃ±os
DistribuciÃ³n gÃ©nero: {'Male': np.int64(41)}
**Perfil Financiero:**
Ingreso anual: $62.3k
Spending Score: 70.2/100

![.](../../assets/image_UT1_p6_31.png)
 
## **ğŸ”Â Paso 4.2: AnÃ¡lisis Silhouette Detallado**

**Pistas:**

- [DocumentaciÃ³n sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)Â - MÃ©tricas de evaluaciÃ³n
- [DocumentaciÃ³n silhouette_samples](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html)Â - Silhouette score por muestra individual
- Â¿QuÃ© funciÃ³n calcula el silhouette score para cada muestra individual?

![.](../../assets/image_UT1_p6_32.png)
 
## **ğŸ”Â Paso 4.3: IdentificaciÃ³n de Outliers**

![.](../../assets/image_UT1_p6_34.png)
 
## **ğŸ”Â Paso 4.4: AnÃ¡lisis de Perfiles de Cliente**

![.](../../assets/image_UT1_p6_33.png)
 
# **ğŸ“Â REFLEXIONES FINALES Y ENTREGABLES**

## ğŸ“Â **Reflexiones de Data Detective**

**Completa estas reflexiones basadas en tu anÃ¡lisis:**

### **ğŸ” MetodologÃ­a CRISP-DM:**

1. Â¿QuÃ© fase fue mÃ¡s desafiante y por quÃ©?Â 

La fase de Modeling, porque hubo que comparar criterios distintos (Elbow vs Silhouette) y balancearlos con la intuiciÃ³n de negocio para elegir el nÃºmero de clusters.

1. Â¿CÃ³mo el entendimiento del negocio influyÃ³ en tus decisiones tÃ©cnicas?Â 

InfluyÃ³ en que, aunque Silhouette sugerÃ­a K=2, se optÃ³ por K=4 porque reflejaba mejor la segmentaciÃ³n esperada en marketing (perfiles alto/bajo ingreso y alto/bajo gasto).

### **ğŸ§¹ Data Preparation:**

1. Â¿QuÃ© scaler funcionÃ³ mejor y por quÃ©?

MinMaxScaler, porque equilibrÃ³ las escalas de ingreso y score de gasto, evitando que una variable dominara el clustering

1. PCA o Feature Selection fue mÃ¡s efectivo para tu caso?

PCA, ya que con 2 componentes explicÃ³ el 86% de la varianza y mejorÃ³ el Silhouette Score a 0.686

1. Â¿CÃ³mo balanceaste interpretabilidad vs performance?

Se eligiÃ³ PCA para performance, pero se conservaron insights de las variables originales (Income y Spending) para la interpretaciÃ³n de negocio.

### **ğŸ§© Clustering:**

1. Â¿El Elbow Method y Silhouette coincidieron en el K Ã³ptimo?

No. Elbow sugerÃ­a 6 clusters, Silhouette sugerÃ­a 2; se eligiÃ³ 4 como punto medio considerando el negocio

1. Â¿Los clusters encontrados coinciden con la intuiciÃ³n de negocio?

SÃ­, aparecieron segmentos de alto ingresoâ€“alto gasto, alto ingresoâ€“bajo gasto, bajo ingresoâ€“alto gasto y bajo ingresoâ€“bajo gasto.

1. Â¿QuÃ© harÃ­as diferente si fueras a repetir el anÃ¡lisis?

ProbarÃ­a otros algoritmos de clustering (DBSCAN, Gaussian Mixtures) y tÃ©cnicas de reducciÃ³n alternativas (t-SNE, UMAP) para validar la robustez de los clusters.

### **ğŸ’¼ AplicaciÃ³n PrÃ¡ctica:**

1. Â¿CÃ³mo presentarÃ­as estos resultados en un contexto empresarial?

Con visualizaciones claras (scatter plots de clusters, perfiles resumidos de cada grupo) y ejemplos de campaÃ±as de marketing dirigidas a cada segmento.

1. Â¿QuÃ© valor aportan estas segmentaciones?

Permiten personalizar ofertas, optimizar la inversiÃ³n publicitaria y mejorar la experiencia del cliente.

1. Â¿QuÃ© limitaciones tiene este anÃ¡lisis?

Se basa en pocas variables (edad, gÃ©nero, ingreso, score), no incluye datos de compras reales ni preferencias especÃ­ficas; ademÃ¡s, el clustering puede variar con diferentes tÃ©cnicas o escalas.
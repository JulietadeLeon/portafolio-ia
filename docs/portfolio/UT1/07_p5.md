---
title: "PrÃ¡ctica 5"
date: 2025-01-01
---

# **PrÃ¡ctica 5: ValidaciÃ³n y SelecciÃ³n de Modelos**

- [Consigna](https://juanfkurucz.com/ucu-ia/ut1/05-validacion-seleccion-modelos/)
- [Google Colab](https://colab.research.google.com/drive/1nn2vaW-PfJkMZOWpS8qGhQc6LWbnKCbT?usp=sharing)

# Dataset: PredicciÃ³n de Ã‰xito Estudiantil

## **Setup Inicial**

- ğŸ”—Â **RidgeClassifier**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)Â - Clasificador con regularizaciÃ³n L2
- ğŸ”—Â **RandomForestClassifier**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)Â - Ensemble de Ã¡rboles de decisiÃ³n
- ğŸ”—Â **cross_val_score**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)Â - Para validaciÃ³n cruzada automÃ¡tica
- ğŸ”—Â **StratifiedKFold**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)Â - KFold que mantiene proporciÃ³n de clases
- ğŸ”—Â **StandardScaler**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)Â - Estandariza caracterÃ­sticas (media=0, std=1)

## **Carga y ExploraciÃ³n de los Datos de Estudiantes**

https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success

**Contexto de negocio:**

- **Problema**: Predecir abandono estudiantil y Ã©xito acadÃ©mico en educaciÃ³n superior
- **Objetivo**: Identificar estudiantes en riesgo para implementar estrategias de apoyo
- **Variables**: 36 caracterÃ­sticas (demogrÃ¡ficas, acadÃ©micas, socioeconÃ³micas)
- **Valor**: Reducir tasas de abandono, mejorar retenciÃ³n estudiantil

**ğŸ“– InvestigaciÃ³n previa:**Â Antes de continuar, explora el dataset en:Â [Student Dropout and Academic Success](https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success)

**Preguntas para investigar:**Â - Â¿CuÃ¡ntas muestras y caracterÃ­sticas tiene el dataset? - Â¿QuÃ© tipos de variables incluye? (demogrÃ¡ficas, acadÃ©micas, socioeconÃ³micas) - Â¿Las clases estÃ¡n balanceadas o desbalanceadas? - Â¿QuÃ© significan las 3 categorÃ­as objetivo?

```python
Dataset: Student Dropout and Academic Success
Estudiantes: 4424, CaracterÃ­sticas: 36
Objetivo: Predecir 1 variable(s)

Variable objetivo: Target

DistribuciÃ³n de resultados acadÃ©micos:

Primeras caracterÃ­sticas:
['Marital Status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', "Mother's qualification", "Father's qualification"] ...

Age at enrollment:
  Promedio: 23.3 aÃ±os
  Rango: 17-70 aÃ±os
```

**Tipos de variables incluidas**

El dataset mezcla distintas dimensiones:

- **DemogrÃ¡ficas:**
    - Estado civil, nacionalidad, edad al momento de inscribirse, gÃ©nero (si estuviera).
- **AcadÃ©micas:**
    - Modalidad (diurno/nocturno), curso, calificaciÃ³n previa, orden de aplicaciÃ³n, asistencia.
    - Notas de exÃ¡menes parciales y finales, rendimiento acumulado.
- **SocioeconÃ³micas / familiares:**
    - Nivel educativo de madre y padre, ocupaciÃ³n de los padres, becas, ayudas financieras, etc.

**Balance de clases**

El *target* estÃ¡ **desbalanceado**:

- La mayorÃ­a de los estudiantes terminan en la categorÃ­a **â€œGraduateâ€** (egresados).
- Una parte significativa son **â€œDropoutâ€** (deserciÃ³n).
- Una minorÃ­a queda como **â€œEnrolledâ€** (todavÃ­a cursando al final del perÃ­odo).

*(La distribuciÃ³n exacta es algo asÃ­ como ~50% graduados, ~30% desertores, ~20% aÃºn inscritos; depende de la fuente original UCI.)*

**Significado de las 3 categorÃ­as objetivo (`Target`)**

1. **Graduate** â†’ el estudiante completÃ³ la carrera con Ã©xito.
2. **Dropout** â†’ el estudiante abandonÃ³ antes de terminar.
3. **Enrolled** â†’ el estudiante sigue inscrito en el sistema (no abandonÃ³ ni se graduÃ³ al final del perÃ­odo observado).

# Cross-Validation - ValidaciÃ³n Robusta

## **PreparaciÃ³n de datos para la validaciÃ³n**

```python
Datos preparados para validaciÃ³n:
X shape: (4424, 36)
y shape: (4424,)
Clases Ãºnicas: [np.int64(0), np.int64(1), np.int64(2)]
Mapeo: {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
```

# Cross-Validation - ValidaciÃ³n Robusta

## **ImplementaciÃ³n de ValidaciÃ³n Cruzada**

- ğŸ”—Â [DocumentaciÃ³n KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)
- ğŸ”—Â [DocumentaciÃ³n StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)
- ğŸ”—Â [DocumentaciÃ³n cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)
- ğŸ’­ Â¿CuÃ¡l mÃ©todo mantiene la proporciÃ³n de clases en cada fold?
- ğŸ“Š Â¿CuÃ¡l parÃ¡metro especifica el tipo de validaciÃ³n cruzada enÂ `cross_val_score`?

```python
ğŸ”¬ VALIDACIÃ“N CRUZADA: Â¿QuÃ© tan estable es nuestro modelo?
Pipeline creado para validaciÃ³n cruzada

KFOLD RESULTS:
   Scores individuales: [0.75254237 0.76610169 0.76836158 0.77740113 0.78054299]
   Media: 0.7690
   DesviaciÃ³n estÃ¡ndar: 0.0098
   Resultado: 0.7690 Â± 0.0098

STRATIFIED KFOLD RESULTS:
   Scores individuales: [0.76836158 0.76836158 0.76271186 0.75480226 0.75452489]
   Media: 0.7618
   DesviaciÃ³n estÃ¡ndar: 0.0061
   Resultado: 0.7618 Â± 0.0061

COMPARACIÃ“N DE ESTABILIDAD:
   StratifiedKFold es MÃS ESTABLE (menor variabilidad)
 
```

![.](../../assets/image_UT1_p5_1.png)

**Resultados obtenidos**

- **KFold**
    - Media â‰ˆ **0.7690**
    - DesvÃ­o estÃ¡ndar â‰ˆ **0.0098**
    - ğŸ“ˆ Tiene un **score promedio mÃ¡s alto**, pero con mayor dispersiÃ³n (mÃ¡s inestable).
    - El boxplot lo muestra con una caja mÃ¡s alta y bigotes mÃ¡s largos â†’ los resultados fluctÃºan bastante segÃºn cÃ³mo se dividen los datos.
- **StratifiedKFold**
    - Media â‰ˆ **0.7618**
    - DesvÃ­o estÃ¡ndar â‰ˆ **0.0061**
    - ğŸ“‰ El **score promedio es un poco mÃ¡s bajo**, pero mucho mÃ¡s **consistente** entre pliegues.
    - El boxplot lo muestra con caja y bigotes mÃ¡s pequeÃ±os â†’ menos variabilidad, mÃ¡s estabilidad.

**InterpretaciÃ³n**

- **KFold normal** puede dar resultados mÃ¡s altos, pero **es mÃ¡s sensible a la forma en que se dividen los datos**.
- **StratifiedKFold**, al mantener la proporciÃ³n de clases en cada pliegue, asegura una evaluaciÃ³n **mÃ¡s representativa y estable**, aunque el promedio sea un poco menor.

**ConclusiÃ³n:**

Si lo mÃ¡s importante es la **estabilidad y confiabilidad de la evaluaciÃ³n**, **StratifiedKFold** es mejor.

Si buscas un **score mÃ¡ximo posible** (aunque mÃ¡s variable), **KFold** puede parecer mÃ¡s atractivo.

# ğŸ† Parte 3: ComparaciÃ³n de Modelos - Â¡El Torneo!

## ğŸ¥ŠÂ **Paso 5: Competencia de MÃºltiples Modelos**

- ğŸ”—Â [DocumentaciÃ³n RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)
- ğŸ”—Â [DocumentaciÃ³n RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
- ğŸ’­ Random Forest no necesita escalado, Â¿por quÃ© no incluir StandardScaler?
- ğŸ“Š Â¿CuÃ¡l mÃ©trica es mejor para clasificaciÃ³n: 'accuracy', 'precision', o 'f1'?
- ğŸ” Los mÃ©todosÂ `.mean()`Â yÂ `.std()`Â funcionan con arrays de numpy

```python
ğŸ† TORNEO: Â¿CuÃ¡l modelo funciona mejor para diagnÃ³stico mÃ©dico?
Modelos en competencia: ['Logistic Regression', 'Ridge Classifier', 'Random Forest']

EVALUANDO MODELOS CON 5-FOLD CV...
   Evaluando Logistic Regression...
   Logistic Regression: 0.7618 Â± 0.0061
      Scores: ['0.768', '0.768', '0.763', '0.755', '0.755']
   Evaluando Ridge Classifier...
   Ridge Classifier: 0.7509 Â± 0.0032
      Scores: ['0.755', '0.746', '0.754', '0.749', '0.751']
   Evaluando Random Forest...
   Random Forest: 0.7658 Â± 0.0064
      Scores: ['0.775', '0.764', '0.771', '0.763', '0.757']

RESULTADOS FINALES:
GANADOR: Random Forest
Score: 0.7658

ANÃLISIS DE ESTABILIDAD:
   Logistic Regression: MUY ESTABLE (std: 0.0061)
   Ridge Classifier: MUY ESTABLE (std: 0.0032)
   Random Forest: MUY ESTABLE (std: 0.0064)
/tmp/ipython-input-2400318206.py:79: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.
  plt.boxplot([results[name] for name in models.keys()],
```

![.](../../assets/image_UT1_p5_2.png)

**InterpretaciÃ³n de los resultados**

1. **Rendimiento promedio (Accuracy):**
    - **Random Forest** obtuvo el mejor score promedio (`0.7658`) â†’ lo posiciona como **ganador**.
    - **Logistic Regression** quedÃ³ muy cerca (`0.7618`), con apenas -0.004 de diferencia.
    - **Ridge Classifier** fue el mÃ¡s bajo (`0.7509`).
2. **Estabilidad (DesviaciÃ³n EstÃ¡ndar):**
    - Los tres modelos fueron **muy estables**, con std < 0.01.
    - **Ridge Classifier** tuvo la menor desviaciÃ³n (`0.0032`), lo que indica que su rendimiento cambia menos entre folds, aunque su accuracy sea menor.
    - Random Forest y Logistic son igualmente estables (`0.0061 â€“ 0.0064`), con pequeÃ±as variaciones entre folds.
3. **Boxplot (izquierda):**
    - Muestra la distribuciÃ³n de accuracy en los 5 folds.
    - Random Forest tiene la mediana mÃ¡s alta y valores superiores consistentes.
    - Logistic Regression tiene un rango similar, con algo mÃ¡s de dispersiÃ³n hacia abajo.
    - Ridge estÃ¡ siempre por debajo, sin alcanzar los niveles de los otros dos.
4. **Barras con error bars (derecha):**
    - Refuerza la idea: todos muy parejos, pero Random Forest saca una ventaja leve en promedio.
    - La diferencia no es enorme, pero sÃ­ consistente.

**ConclusiÃ³n**

- **Random Forest** es el modelo con **mejor rendimiento global** en este dataset (diagnÃ³stico mÃ©dico).
- **Logistic Regression** es una alternativa muy cercana, mÃ¡s simple e interpretable.
- **Ridge Classifier** es el mÃ¡s dÃ©bil, aunque con la mayor estabilidad.

### **ğŸ“šÂ BONUS: Â¿QuÃ© significan las mÃ©tricas de validaciÃ³n?**

- ğŸ”Â [GuÃ­a completa de cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)
- ğŸ“Š Un modelo estable tiene baja o alta variabilidad?
- ğŸ©º En medicina, Â¿prefieres un modelo consistente o uno que varÃ­a mucho?

**Cross-Validation:** TÃ©cnica que divide los datos en **k partes (folds)** para entrenar y evaluar mÃºltiples veces.

**Accuracy promedio:** La **estimaciÃ³n** de rendimiento esperado en datos nuevos.

**DesviaciÃ³n estÃ¡ndar:** Indica quÃ© tan **estable o variable** es el modelo entre diferentes divisiones de datos.

 Un modelo **estable** tiene **baja variabilidad**.

**StratifiedKFold:** Mantiene la **proporciÃ³n** de clases en cada fold, especialmente importante en datasets desbalanceados.

# ğŸš€ BONUS: OptimizaciÃ³n de HiperparÃ¡metros

## **GridSearchCV vs RandomizedSearchCV**

- ğŸ”—Â **GridSearchCV**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)Â - BÃºsqueda exhaustiva de hiperparÃ¡metros
- ğŸ”—Â **RandomizedSearchCV**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)Â - BÃºsqueda aleatoria mÃ¡s eficiente
- ğŸ’­Â **GridSearchCV**Â prueba todas las combinaciones posibles
- ğŸ’­Â **RandomizedSearchCV**Â es mÃ¡s rÃ¡pido para espacios grandes de bÃºsqueda
- ğŸ’­Â **n_jobs=-1**Â usa todos los procesadores disponibles

```python
Optimizando hiperparÃ¡metros para: Random Forest

MÃ©todo 1: GridSearchCV (bÃºsqueda exhaustiva)
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Mejores parÃ¡metros (Grid): {'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}
Mejor score (Grid): 0.7783

MÃ©todo 2: RandomizedSearchCV (bÃºsqueda aleatoria)
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Mejores parÃ¡metros (Random): {'classifier__n_estimators': 100, 'classifier__min_samples_split': 5, 'classifier__max_depth': 30}
Mejor score (Random): 0.7783

ComparaciÃ³n de eficiencia:
GridSearch probÃ³: 36 combinaciones
RandomSearch probÃ³: 20 combinaciones

Modelo final optimizado: 0.7783 Â± 0.0067
```

La salida muestra cÃ³mo se comportaron los dos mÃ©todos de bÃºsqueda de hiperparÃ¡metros al optimizar un **Random Forest**:

**MÃ©todo 1: GridSearchCV (bÃºsqueda exhaustiva)**

- ProbÃ³ **todas las combinaciones posibles** de los hiperparÃ¡metros definidos (36 en total).
- El mejor conjunto fue:
    - `n_estimators = 100`
    - `max_depth = None` (Ã¡rboles sin lÃ­mite de profundidad)
    - `min_samples_split = 5` (cada nodo debe tener al menos 5 muestras para dividirse).
- Score medio en validaciÃ³n cruzada: **0.7783**.

**MÃ©todo 2: RandomizedSearchCV (bÃºsqueda aleatoria)**

- ProbÃ³ solo **20 combinaciones aleatorias** de los mismos hiperparÃ¡metros.
- El mejor conjunto fue ligeramente distinto:
    - `n_estimators = 100`
    - `max_depth = 30`
    - `min_samples_split = 5`.
- Score medio en validaciÃ³n cruzada: **0.7783** (idÃ©ntico al obtenido por GridSearch).

**ComparaciÃ³n de eficiencia**

- GridSearch necesitÃ³ evaluar **36 combinaciones Ã— 5 folds = 180 entrenamientos**.
- RandomizedSearch usÃ³ **20 combinaciones Ã— 5 folds = 100 entrenamientos**.
- **Ambos llegaron al mismo rendimiento final**, pero RandomizedSearch lo hizo con **menos cÃ¡lculos** (mÃ¡s eficiente).

**ConclusiÃ³n**

- El rendimiento mÃ¡ximo alcanzado con ambos mÃ©todos fue **~77.8% de accuracy**.
- La diferencia en los hiperparÃ¡metros Ã³ptimos (`max_depth=None` vs `max_depth=30`) no afectÃ³ el resultado en este caso.
- Para espacios pequeÃ±os de bÃºsqueda, **GridSearch garantiza la mejor combinaciÃ³n exacta**.
- Para espacios grandes, **RandomizedSearch puede encontrar resultados igual de buenos con menos coste computacional**.

### **Â¿CuÃ¡ndo usar cada mÃ©todo?**

**GuÃ­a de decisiÃ³n**

- **GridSearchCV** cuando tienes **pocos hiperparÃ¡metros** y **suficiente tiempo de cÃ³mputo**.
- **RandomizedSearchCV** cuando tienes **muchos hiperparÃ¡metros** o **tiempo limitado**.
- **Pipeline + SearchCV** siempre previene **data leakage automÃ¡ticamente**.
- **cross_val_score** en el resultado final valida que la optimizaciÃ³n no causÃ³ **overfitting**.

# ğŸ” BONUS 2: Explicabilidad del Modelo

## **Â¿Por quÃ© el modelo toma esas decisiones?**

- ğŸ”—Â **Feature Importance**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)Â - QuÃ© caracterÃ­sticas son mÃ¡s importantes
- ğŸ”—Â **plot_tree**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)Â - VisualizaciÃ³n grÃ¡fica de Ã¡rboles individuales
- ğŸ”—Â **export_text**:Â [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html)Â - RepresentaciÃ³n en texto de Ã¡rboles
- ğŸ’­Â **Feature Importance**Â se basa en cuÃ¡nto reduce cada caracterÃ­stica la impureza
- ğŸ’­Â **Cada Ã¡rbol**Â en Random Forest es diferente debido a bootstrap + random feature selection
- ğŸ’­Â **max_depth=3**Â limita la visualizaciÃ³n para que sea legible
- ğŸ’­Â **Diversidad**Â entre Ã¡rboles es lo que hace potente al Random Forest

```python
Usando el modelo ganador para explicabilidad
Componentes del pipeline: ['classifier']

TOP 10 CARACTERÃSTICAS MÃS IMPORTANTES:
Curricular units 2nd sem (approved): 0.1516
Curricular units 2nd sem (grade): 0.1193
Curricular units 1st sem (approved): 0.0987
Curricular units 1st sem (grade): 0.0589
Tuition fees up to date: 0.0466
Curricular units 2nd sem (evaluations): 0.0419
Admission grade: 0.0385
Age at enrollment: 0.0372
Curricular units 1st sem (evaluations): 0.0349
Previous qualification (grade): 0.0343
```

![.](../../assets/image_UT1_p5_3.png)

```python
IMPORTANCIA POR CATEGORÃAS:
Factores acadÃ©micos: 0.6443
Factores demogrÃ¡ficos: 0.0499
Factores econÃ³micos: 0.0769

INTERPRETACIÃ“N PARA INTERVENCIONES:
La caracterÃ­stica mÃ¡s importante es: Curricular units 2nd sem (approved)
Esto sugiere que para reducir abandono estudiantil debemos enfocarnos en:
1. Monitorear y mejorar: Curricular units 2nd sem (approved)
2. Monitorear y mejorar: Curricular units 2nd sem (grade)
3. Monitorear y mejorar: Curricular units 1st sem (approved)

ANÃLISIS DE ESTUDIANTE INDIVIDUAL (ejemplo):
Estudiante #0:
PredicciÃ³n: Dropout
Probabilidades:
  Dropout: 0.737
  Enrolled: 0.082
  Graduate: 0.181

Top 5 caracterÃ­sticas que influyen en esta predicciÃ³n:
Curricular units 2nd sem (approved): 0.00 (importancia: 0.1516)
Curricular units 2nd sem (grade): 0.00 (importancia: 0.1193)
Curricular units 1st sem (approved): 0.00 (importancia: 0.0987)
Curricular units 1st sem (grade): 0.00 (importancia: 0.0589)
Tuition fees up to date: 1.00 (importancia: 0.0466)

VISUALIZACIÃ“N DE ÃRBOLES DEL RANDOM FOREST:
Mostrando 3 Ã¡rboles de 100 totales
```

![.](../../assets/image_UT1_p5_4.png)

```python
ESTADÃSTICAS DE LOS ÃRBOLES:
Profundidad promedio (primeros 5 Ã¡rboles): 21.2
NÃºmero promedio de nodos (primeros 5): 1139.0

EJEMPLO DE REGLAS DE DECISIÃ“N (Ãrbol 1, simplificado):
|--- Curricular units 2nd sem (approved) <= 3.50
|   |--- Curricular units 2nd sem (evaluations) <= 7.50
|   |   |--- Curricular units 1st sem (enrolled) <= 0.50
|   |   |   |--- truncated branch of depth 10
|   |   |--- Curricular units 1st sem (enrolled) >  0.50
|   |   |   |--- truncated branch of depth 10
|   |--- Curricular units 2nd sem (evaluations) >  7.50
|   |   |--- Age at enrollment <= 25.50
|   |   |   |--- truncated branch of depth 16
|   |   |--- Age at enrollment >  25.50
|   |  ...

DIVERSIDAD EN EL RANDOM FOREST:
El poder del Random Forest viene de la diversidad de sus Ã¡rboles:
- Cada Ã¡rbol ve una muestra diferente de datos (bootstrap)
- Cada split considera solo un subconjunto aleatorio de caracterÃ­sticas
- La predicciÃ³n final es el voto mayoritario de todos los Ã¡rboles
Usando datos sin escalar para Ã¡rboles individuales

Predicciones de Ã¡rboles individuales para el Estudiante #0:
  Ãrbol 1: 2.0
  Ãrbol 2: 0.0
  Ãrbol 3: 0.0
  Ãrbol 4: 0.0
  Ãrbol 5: 0.0
PredicciÃ³n final (voto mayoritario): 0.0
```

### **AnÃ¡lisis de explicabilidad**

**Importancia global de caracterÃ­sticas**

- El modelo identificÃ³ que las variables acadÃ©micas tienen **el mayor peso (64%)** en las predicciones, muy por encima de factores demogrÃ¡ficos (5%) y econÃ³micos (7%).
- Entre ellas destacan:
    - **Curricular units 2nd sem (approved)** â†’ la mÃ¡s influyente, indica cuÃ¡ntas materias aprobÃ³ en el segundo semestre.
    - **Curricular units 2nd sem (grade)** y **1st sem (approved)** tambiÃ©n son claves.
- **InterpretaciÃ³n prÃ¡ctica:** el Ã©xito acadÃ©mico temprano (primer y segundo semestre) es determinante para evitar la deserciÃ³n.

**Intervenciones sugeridas**

1. **Monitorear aprobaciones del 2Â° semestre**: si un estudiante reprueba muchas, estÃ¡ en riesgo.
2. **AcompaÃ±ar en notas (grades)** desde el 1Â° y 2Â° semestre â†’ tutorÃ­as, apoyo docente.
3. **Controlar cumplimiento de pagos (tuition fees)**, aunque su importancia es menor, puede anticipar problemas financieros que lleven al abandono.

**AnÃ¡lisis individual (ejemplo Estudiante #0)**

- **PredicciÃ³n:** Dropout (abandono).
- **Probabilidades:** Dropout 73.7% vs Graduate 18.1% vs Enrolled 8.2%.
- **Razones de la predicciÃ³n:**
    - No aprobÃ³ ni rindiÃ³ materias en 1er y 2do semestre â†’ el modelo lo ve como altÃ­simo riesgo.
    - A pesar de tener las cuotas pagas (tuition fees up to date = 1), pesa mucho mÃ¡s lo acadÃ©mico.
- Esto es Ãºtil para **accionar sobre un estudiante en particular** â†’ identificarlo como â€œcrÃ­ticoâ€ y aplicar medidas preventivas.

**ExplicaciÃ³n de los Ã¡rboles**

- Cada Ã¡rbol genera reglas de decisiÃ³n del tipo *â€œSi aprobÃ³ â‰¤ 3 materias y rindiÃ³ â‰¤ 7 evaluaciones, entonces riesgo de abandono altoâ€*.
- El bosque combina **100 Ã¡rboles**, cada uno con reglas diferentes.
- **Profundidad promedio ~21** y mÃ¡s de 1000 nodos â†’ Ã¡rboles complejos, pero el bosque compensa esto con diversidad.

**Diversidad en el Random Forest**

- **Bootstrap:** cada Ã¡rbol ve una muestra distinta de estudiantes.
- **Subconjuntos aleatorios de caracterÃ­sticas:** en cada split no se consideran todas las variables.
- **MayorÃ­a de votos:** cada Ã¡rbol da su predicciÃ³n, y el resultado final es el voto mayoritario.

Ejemplo Estudiante #0:

- Ãrbol 1 â†’ â€œGraduateâ€
- Ãrboles 2,3,4,5 â†’ â€œDropoutâ€
- MayorÃ­a = Dropout â†’ predicciÃ³n final confiable.

**En resumen**

El modelo muestra que **el rendimiento acadÃ©mico inicial es el factor decisivo para anticipar abandono**, mÃ¡s que edad o situaciÃ³n econÃ³mica.

La explicabilidad permite:

- Confiar en que el modelo no es â€œcaja negraâ€.
- DiseÃ±ar intervenciones focalizadas en lo acadÃ©mico.
- Explicar a directivos y docentes **por quÃ©** un alumno aparece en riesgo.

### ğŸ“Œ **Â¿Por quÃ© es importante la explicabilidad?**

- **Confianza:** Los educadores necesitan **entender** por quÃ© el modelo predice abandono.
- **Intervenciones:** Knowing las caracterÃ­sticas importantes permite crear **acciones/intervenciones especÃ­ficas**.
- **Bias detection:** La explicabilidad ayuda a detectar **sesgos** en el modelo.
- **Regulaciones:** Muchos contextos requieren modelos **explicables/transparentes** por ley.
- **Mejora continua:** Entender el modelo ayuda a **mejorar y ajustar** futuras versiones.

# **Preguntas de ReflexiÃ³n**

1. **Â¿QuÃ© es data leakage y por quÃ© es peligroso?**

**ğŸ’¡ PISTA:**Â Piensa en quÃ© informaciÃ³n "ve" el modelo antes de tiempo

- **DefiniciÃ³n:** ocurre cuando el modelo accede a informaciÃ³n en el entrenamiento que **no deberÃ­a tener disponible en la vida real** (ejemplo: usar una variable futura o derivada del target).
- **Peligro:** hace que el modelo parezca tener una performance excelente en validaciÃ³n, pero **fracasa al aplicarse en datos reales**, porque estaba â€œhaciendo trampaâ€ al ver datos del futuro o del test.
1. **Â¿CuÃ¡ndo usar KFold vs StratifiedKFold?**

**ğŸ’¡ PISTA:**Â Â¿QuÃ© pasa si una clase tiene muy pocas muestras?

- **KFold:** divide en K particiones iguales, **sin importar la distribuciÃ³n de clases**.
- **StratifiedKFold:** mantiene la **proporciÃ³n de clases en cada fold** (estratificaciÃ³n).
- **DecisiÃ³n:**
    - UsÃ¡ **KFold** en regresiÃ³n o cuando las clases estÃ¡n bien balanceadas.
    - UsÃ¡ **StratifiedKFold** en clasificaciÃ³n con clases desbalanceadas (pocas muestras en una clase).
1. **Â¿CÃ³mo interpretar "95.2% Â± 2.1%" en cross-validation?**

**ğŸ’¡ PISTA:**Â Â¿QuÃ© significa cada nÃºmero para el rendimiento del modelo?

- **95.2%:** media de accuracy en los distintos folds.
- **Â± 2.1%:** desviaciÃ³n estÃ¡ndar â†’ mide **la variabilidad/estabilidad** del rendimiento entre folds.
- InterpretaciÃ³n: el modelo rinde alrededor de **95%**, con cierta variaciÃ³n (entre 93% y 97%, aprox.).
1. **Â¿Por quÃ© Random Forest no necesita StandardScaler?**

**ğŸ’¡ PISTA:**Â ğŸ”—Â [CÃ³mo funcionan los Ã¡rboles de decisiÃ³n](https://scikit-learn.org/stable/modules/tree.html)

- Los Ã¡rboles de decisiÃ³n (y ensembles como Random Forest) hacen **cortes basados en umbrales de las variables** (ej: `X > 5`).
- No dependen de la magnitud ni escala de las variables â†’ da igual si una estÃ¡ en metros y otra en kilÃ³metros.
- En cambio, modelos como regresiÃ³n logÃ­stica o SVM sÃ­ necesitan estandarizaciÃ³n.
1. **En diagnÃ³stico mÃ©dico, Â¿prefieres un modelo con 98% accuracy pero inestable, o 95% accuracy pero muy estable?**

**ğŸ’¡ PISTA:**Â ğŸ©º Â¿QuÃ© es mÃ¡s importante: mÃ¡ximo rendimiento o confiabilidad?

- En medicina, lo mÃ¡s importante es la **confiabilidad y reproducibilidad**.
- Un modelo inestable puede dar resultados muy distintos segÃºn el dataset â†’ peligroso para decisiones clÃ­nicas.
- Por eso, mejor elegir **95% estable**, aunque no sea el mÃ¡ximo accuracy, porque **consistencia > rendimiento mÃ¡ximo**.

---

**ğŸ“Š Practica con otros datasets:**

- ğŸ Â [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html)Â - RegresiÃ³n con cross-validation
- ğŸ·Â [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)Â - ClasificaciÃ³n multiclase
- ğŸ“§Â [SMS Spam](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)Â - Texto + pipelines
- ğŸŒ¸Â [Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)Â - ClasificaciÃ³n multiclase balanceada

**ğŸ¤– Aprende sobre tÃ©cnicas de validaciÃ³n mÃ¡s avanzadas:**

- âš–ï¸Â [LeaveOneOut](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html)Â - Para datasets pequeÃ±os
- ğŸ“ŠÂ [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)Â - Para datos temporales
- ğŸ”Â [Learning Curves](https://scikit-learn.org/stable/modules/learning_curve.html)Â - Analizar overfitting vs underfitting

**ğŸ“ˆ Explora mÃ©tricas especÃ­ficas para casos mÃ©dicos:**

- ğŸ©ºÂ [ROC-AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)Â - Para problemas de diagnÃ³stico
- ğŸ“ŠÂ [Precision-Recall curves](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)Â - Datasets desbalanceados
- âš–ï¸Â [Balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)Â - Clases desbalanceadas